{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IntelDragon Tailored News Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IntelDragon Tailored News Feed is a scalable artificial intelligence platform that identifies relevant trending cyber threats to an organization, industry, company, or product based on a written description of the topic of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example search query\n",
    "\n",
    "search_query = \"cyber attacks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text\n",
    "\n",
    "input_text = \"\"\"The College of Computing & Informatics is a national leader in information and technology education. With cutting edge curriculum and groundbreaking research led by a world-class faculty, our college is one of the only institutions in the nation that can equip you with the comprehensive knowledge, skills and hands-on experience to drive innovation and improve lives within any industry or career field you choose.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import newspaper\n",
    "import json\n",
    "from pygooglenews import GoogleNews\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import datetime\n",
    "from datetime import datetime, date, timedelta\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from localVariables import stop, article_list\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.today() - timedelta(days=1)\n",
    "\n",
    "gn = GoogleNews()\n",
    "s = gn.search(search_query, from_=(date.today() - timedelta(days=1)).strftime('%Y-%m-%d'), to_=(date.today()).strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_list = []\n",
    "\n",
    "# for i in range(0,len(s[\"entries\"])):\n",
    "#   try:\n",
    "#     url = s[\"entries\"][i][\"link\"]\n",
    "#     article = newspaper.Article(url=url, language='en')\n",
    "#     article.download()\n",
    "#     article.parse()\n",
    "#     article ={\n",
    "#       \"title\": str(article.title),\n",
    "#       \"text\": str(article.text),\n",
    "#       \"authors\": article.authors,\n",
    "#       \"published_date\": str(article.publish_date),\n",
    "#       \"top_image\": str(article.top_image),\n",
    "#       \"videos\": article.movies,\n",
    "#       \"keywords\": article.keywords,\n",
    "#       \"summary\": str(article.summary),\n",
    "#       \"url\": str(url)\n",
    "#     }\n",
    "#     article_list.append(article)\n",
    "#   except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Into the Breach: Breaking Down 3 SaaS App Cyber Attacks in 2022'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_list[0][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Natural Language Processing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "data = []\n",
    "  \n",
    "for a in article_list:\n",
    "    for sentence in a[\"text\"].split(\"\\n\"):\n",
    "        sentence_list = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            sentence_list.append(word)\n",
    "        title.append(a[\"title\"])\n",
    "        data.append(sentence_list)\n",
    "\n",
    "# model = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg = 1)\n",
    "model = gensim.models.Word2Vec.load('Cybersecurity_Unigram_01.bin')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[0-9A-Za-z]*[A-Za-z][0-9A-Za-z]*')\n",
    "def convert_to_tokens(text):\n",
    "    intermediate = tokenizer.tokenize(text)\n",
    "    intermediate = [(i.lower() if i[1:].lower()==i[1:] else i) for i in intermediate]\n",
    "    intermediate = [i for i in intermediate if i not in stop]\n",
    "    return intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get News Article Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = []\n",
    "article_vectors = []\n",
    "article_texts = []\n",
    "article_url = []\n",
    "for article in article_list:\n",
    "    if \"Types of Cyber Attacks:\" in article[\"title\"]:\n",
    "        pass\n",
    "    else:\n",
    "        article_vector = []\n",
    "        sentences = article[\"text\"].split(\"\\n\")\n",
    "        for line in sentences:\n",
    "            line_vector = []\n",
    "            for word in convert_to_tokens(line):\n",
    "                try:\n",
    "                    line_vector.append(model.wv.get_vector(word))\n",
    "                except:\n",
    "                    pass\n",
    "            if len(line_vector)>0:\n",
    "                article_vector.append(np.mean(line_vector, axis=0))\n",
    "                \n",
    "        article_vectors.append(article_vector)\n",
    "        article_titles.append(article[\"title\"])\n",
    "        article_url.append(article[\"url\"])\n",
    "        article_texts.append(article[\"text\"])\n",
    "        \n",
    "# data = np.array(article_vectors) # sentence vectors\n",
    "labels = np.array(article_titles) # article titles\n",
    "texts = np.array(article_texts) # article texts\n",
    "urls = np.array(article_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get User Input Text Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vector = []\n",
    "line_vector = []\n",
    "for sentence in input_text.replace(\"\\n\",\"\").split(\".\"):\n",
    "    line_vector = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            line_vector.append(model.wv.get_vector(word))\n",
    "        except:\n",
    "            pass\n",
    "        if len(line_vector)>0:\n",
    "            sentence_vector = np.mean(line_vector, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Relevant Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYBER SECURITY ENHANCED: NTT DATA BUSINESS SOLUTIONS AND SECURITYBRIDGE EXTEND THEIR PARTNERSHIP\n",
      "0.2830878496170044\n",
      "\n",
      "SmallSat Launch Company Teams with C8 Secure to Provide Cybersecurity Solutions for Space Industry\n",
      "0.40529143810272217\n",
      "\n",
      "Computer Services : Is Your Institution Prepared for the Computer-Security Incident Rule?\n",
      "0.4232533574104309\n",
      "\n",
      "Everbridge (EVBG), Atalait Tie Up to Provide CEM Solutions\n",
      "0.4661410450935364\n",
      "\n",
      "Chinese hackers targeted 7 Indian power hubs, govt says ops failed\n",
      "0.47882723808288574\n",
      "\n",
      "Pacific Institute Water Conflict Chronology Updated\n",
      "0.48011887073516846\n",
      "\n",
      "Aviation Cyber Security Market Is Booming Worldwide with Airbus, BAE Systems â€“ Bloomingprairieonline\n",
      "0.5093559920787811\n",
      "\n",
      "IT Insight: MDR-Managed Detection and Response\n",
      "0.5262599587440491\n",
      "\n",
      "US Cyber Command reinforces Ukraine and allies amid Russian onslaught\n",
      "0.5324267148971558\n",
      "\n",
      "Cyber phases of a hybrid war. Catphishing in Israel. China snoops India's grid. Princes and dissidents.\n",
      "0.5738224387168884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tree = spatial.KDTree(data)\n",
    "\n",
    "# print(labels[tree.query(sentence_vector, k=5)[1][0]])\n",
    "# print(urls[tree.query(sentence_vector, k=5)[1][0]]+\"\\n\")\n",
    "\n",
    "# print(labels[tree.query(sentence_vector, k=5)[1][1]])\n",
    "# print(urls[tree.query(sentence_vector, k=5)[1][1]]+\"\\n\")\n",
    "\n",
    "# print(labels[tree.query(sentence_vector, k=5)[1][2]])\n",
    "# print(urls[tree.query(sentence_vector, k=5)[1][2]]+\"\\n\")\n",
    "\n",
    "# print(labels[tree.query(sentence_vector, k=5)[1][3]])\n",
    "# print(urls[tree.query(sentence_vector, k=5)[1][3]]+\"\\n\")\n",
    "\n",
    "# print(labels[tree.query(sentence_vector, k=5)[1][4]])\n",
    "# print(urls[tree.query(sentence_vector, k=5)[1][4]]+\"\\n\")\n",
    "\n",
    "sorted_articles = []\n",
    "for a in range(0,len(article_vectors)-1):\n",
    "    relevance = 10.0\n",
    "    for sentence in article_vectors[a]:\n",
    "        dist = spatial.distance.cosine(sentence_vector, sentence)\n",
    "        if dist < relevance:\n",
    "            relevance = dist\n",
    "    sorted_articles.append([a, relevance])\n",
    "sorted_articles.sort(key=lambda l: l[1])\n",
    "\n",
    "for a, score in sorted_articles[:10]:\n",
    "    print(article_titles[a])\n",
    "    print(score)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(article_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
